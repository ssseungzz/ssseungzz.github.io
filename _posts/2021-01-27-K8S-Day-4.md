---
title: 쿠버네티스 사용해 보기 3
categories:
  - Infra
tags:
  - k8s
toc: true
toc_label: "Contents"
toc_sticky: true
---

### K8S Day 4 - 쿠버네티스 사용해 보기 3

##### 퍼시스턴스 볼륨(PV)과 퍼시스턴스 볼륨 클레임(PVC)

* 상태가 있는 애플리케이션의 경우에는 데이터를 어떻게 관리해야 할까? 이전 글에서처럼 MySQL 디플로이먼트를 통해 포드를 생성했다고 해도 포드 내부 데이터는 영속적이지 않다. 
* 이를 해결하기 위해 어느 노드에서도 접근해 사용할 수 있는 퍼시스턴스 볼륨을 사용한다. 퍼시스턴트 볼륨은 워커 노드들이 네트워크상에서 스토리지를 마운트에 영속적으로 데이터를 저장할 수 있는 볼륨을 의미한다. 
* 네트워크로 연결해 사용할 수 있는 퍼시스턴트 볼륨의 예로는 NFS, AWS의 EBS(Elastic Block Store), Ceph 등이 있다. 쿠버네티스는 퍼시스턴트 볼륨을 사용하기 위한 기능을 자체적으로 제공한다.

###### 로컬 볼륨: hostPath, emptyDir

* `hostPath`는 볼륨을 호스트와 공유하기 위해 사용하고 `emptyDir`은 포드의 컨테이너 간에 볼륨을 공유하기 위해서 사용한다. 먼저 `hostPath`는 호스트와 디렉터리를 공유한다.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-pod
spec:
  containers:
    - name: my-container
      image: busybox
      args: ["tail", "-f", "/dev/null"]
      volumeMounts:
      - name: my-hostpath-volume
        mountPath: /etc/data
  volumes:
    - name: my-hostpath-volume
      hostPath:
        path: /tmp
```

* 볼륨에서 `hostPath` 항목을 정의해서 호스트의 `/tmp`를 포드의 `/etc/data`에 마운트한다. 
* 그러나 이러한 방식의 보존은 바람직하지 않다. 디플로이먼트의 포드에 장애가 생겨 다른 노드로 옮겨 갔을 경우 이전 노드에 저장된 데이터를 사용할 수 없다. 
  * `hostPath` 볼륨은 모든 노드에 배치해야 하는 특수한 포드의 경우에는 유용하게 사용할 수 있다. 
* `emptyDir` 볼륨은 포드의 데이터를 영속적으로 보존하기 위해 외부 볼륨을 사용하는 것이 아니라 포드가 실행되는 도중에만 필요한 휘발성 데이터를 각 컨테이너가 함께 사용할 수 있도록 임시 저장 공간을 생성한다. 포드가 삭제되면 데이터도 함께 사라진다.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: emptydir-pod
spec:
  containers:
  - name: content-creator
    image: test/alpine-wget:latest
    args: ["tail", "-f", "/dev/null"]
    volumeMounts:
    - name: my-emptydir-volume
      mountPath: /data
  - name: apache-webserver
    image: httpd:2
    volumeMounts:
    - name: my-emptydir-volume
      mountPath: /usr/local/apache2/htdocs
  volumes:
    - name: my-emptydir-volume
      emptyDir: {}
```

* `moundPath`의 디렉터리의 내용은 공유된다.

###### 네트워크 볼륨

* 쿠버네티스는 별도 플러그인 설치 없이 다양한 종류의 네트워크 볼륨을 포드에 마운트할 수 있다. 네트워크 볼륨의 위치는 특별히 정해진 것이 없으며, 네트워크로 접근할 수 있다면 쿠버네티스 클러스터 내부, 외부 어느 곳에 존재해도 크게 상관없다. 

  * 그러나 클라우드에 종속적인 볼륨을 사용하려면 쿠버네티스 클러스터를 생성할 때 특정 클라우드를 위한 옵션이 별도로 설정돼 있어야 한다.

* NFS(Network File System)를 네트워크 볼륨으로 사용하기

  * 대부분의 운영 체제에서 사용할 수 있는 네트워크 스토리지로 여러 개의 클라이언트가 동시에 마운트해 사용할 수 있다. NFS 서버와 클라이언트가 각각 필요하다. 서버는 영속적인 데이터가 실제로 저장되는 네트워크 스토리지 서버이고 클라이언트는 서버에 마운트에 스토리지에 파일을 읽고 쓰는 역할이다. 

  ```yaml
  # nfs-deployment
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: nfs-server
  spec:
    selector:
      matchLabels:
        role: nfs-server
    template:
      metadata:
        labels:
          role: nfs-server
      spec:
        containers:
        - name: nfs-server
          image: gcr.io/google_containers/volume-nfs:0.8
          ports:
            - name: nfs
              containerPort: 2049
            - name: mountd
              containerPort: 20048
            - name: rpcbind
              containerPort: 111
          securityContext:
            privileged: true
  ```

  ```yaml
  # nfs-service
  apiVersion: v1
  kind: Service
  metadata: nfs-service
  spec:
    ports:
    - name: nfs
      port: 2049
    - name: mountd
      port: 20048
    - name: rpcbind
      port: 111
   selector:
     role: nfs-server
  ```

  ```yaml
  # nfs-pod
  apiVersion: v1
  kind: Pod
  metadata:
    name: nfs-pod
  spec:
    containers:
      - name: nfs-mount-container
        image: busybox
        args: ["tail", "-f", "/dev/null"]
        volumeMounts:
        - name: nfs-volume
          mountPath: /mnt
    volumes:
    - name: nfs-volume
      nfs: # NFS 서버의 볼륨을 포드의 컨테이너에 마운트한다.
        path: /
        server: {NFS_SERVER_IP}
  ```

  * `moundPath` 설정으로 인해 NFS 서버의 네트워크 볼륨은 포드 컨테이너의 `/mnt` 디렉터리에 마운트된다. `server` 항목이 서비스의 DNS 이름이 아닌 `{NFS_SERVER_IP}`로 설정돼 있는 것에 주의해야 한다. NFS 볼륨의 마운트는 컨테이너 내부가 아닌 워커 노드에서 발생하므로 서비스의 DNS 이름으로 접근할 수 없다. 노드에서는 포드의 IP로 통신은 할 수 있지만 쿠버네티스의 DNS를 사용하도록 설정돼 있지는 않다. 

###### PV, PVC를 이용한 볼륨 관리

* 애플리케이션을 배포하는 YAML 파일에 네트워크 스토리지 종류를 직접 명시하는 경우 볼륨과 애플리케이션의 정의가 서로 밀접하게 연관돼 있어 분리할 수 없는 상황이 된다.
* 쿠버네티스는 이러한 불편함을 해결하기 위해 퍼시스턴트 볼륨(Persistent Volume)과 퍼시스턴트 볼륨 클레임(Persistent Volume Claim)이라는 오브젝트를 제공한다. 이 두 오브젝트는 포드가 볼륨의 세부적인 정보를 몰라도 볼륨을 사용할 수 있도록 추상화해 주는 역할을 담당한다. 
* 쿠버네티스 클러스터를 관리하는 인프라 관리자와 애플리케이션을 배포하려는 사용자(개발자)가 나뉘어 있다고 할 때 인프라 관리자는 NFS, Ceph와 같은 스토리지에 접근해 사용할 수 있고 이를 쿠버네티스로 가져오는 역할을 담당한다. 이때, 사용자가 디플로이먼트의 포드에 볼륨을 마운트해 사용하려면 다음과 같은 과정을 거치게 된다.
  1. 인프라 관리자는 네트워크 볼륨의 정보를 이용해 퍼시스턴트 볼륨 리소스를 미리 생성해 둔다. 네트워크 볼륨의 정보에는 NFS나 iSCSI 같은 스토리지 서버에 마운트하기 위한 엔드포인트가 포함될 수 있다.
  2. 사용자는 포드를 정의하는 YAML 파일에 '이 포드는 데이터를 영속적으로 저장해야 하므로 마운트할 수 있는 외부 볼륨이 필요하다'라는 의미의 퍼시스턴트 볼륨 클레임을 명시하고, 해당 퍼시스턴트 볼륨 클레임을 생성한다.
  3. 쿠버네티스는 기존에 인프라 관리자가 생성해 뒀던 퍼시스턴트 볼륨의 속성과 사용자가 요청한 퍼시스턴트 볼륨 클레임의 요구 사항이 일치한다면 두 개의 리소스를 매칭시켜 바인드한다. 포드가 이 퍼시스턴트 볼륨 클레임을 사용함으로써 포드의 컨테이너 내부에 볼륨이 마운트된 상태로 생성됩낟.
* 사용자는 **디플로이먼트 YAML 파일에 볼륨의 상세한 스펙을 정의하지 않아도 된다**. 

* GKE에서 영구 디스크라는 GCP의 볼륨을 사용해 보자.

  1. GCP에서 Persistent Disk를 생성한다.
  2. 해당 Persistent Disk를 GKE에서 퍼시스턴트 볼륨으로서 등록한다.
  3. 퍼시스턴스 볼륨 클레임을 포드와 함께 생성한다.

  * 먼저, 대략 5GB 크기의 Persistent Disk를 생성한다. `--zone`에는 GKE 클러스터가 위치한 zone을 입력한다.

  ```
  gcloud compute disks create --type=pd-standard --size=10GB --zone=us-central1-c my-persistent-disk
  ```

  * 방금 생성한 디스크를 GKE의 퍼시스턴트 볼륨으로 등록하기 위해 YAML 파일을 작성한다. `pdName`에는 방금 생성한 디스크의 이름을 입력한다. 이를 쿠버네티스에 `kubectl apply -f`로 적용 후 퍼시스턴트 볼륨이 생성된 것을 확인한다.

  ```yaml
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: my-pv
  spec:
    accessModes:
      - ReadWriteOnce # 하나의 포드(또는 인스턴스)에 의해서만 파운트될 수 있음
    capacity:
      storage: 10Gi # 볼륨의 크기 명시
    gcePersistentDist:
      pdName: my-persistent-disk
  ```

  * 퍼시스턴트 볼륨 생성이 확인되면 퍼시스턴트 볼륨 클레임을 생성해 준다. 퍼시스턴트 볼륨 클레임은 `accessMode`, 스토리지 크기 등을 명시함으로써 원하는 퍼시스턴트 볼륨과 연결된다.

  ```yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: my-pvc
  spec:
    storageClassName: "" # 다이나믹 프로비저닝을 사용하지 않음을 명시
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 5Gi
  ---
  apiVersion: v1
  kind: Pod
  metadata:
    name: pd-mount-container
  spec:
    containers:
      - name: pd-mount-container
        image: busybox
        args: [ "tail", "-f", "/dev/null" ]
        volumeMounts:
        - name: pd-volume
          mountPath: /mnt
    volumes:
    - name : pd-volume
      persistentVolumeClaim:
        claimName: my-pvc # my-pvc 라는 퍼시스턴트 볼륨 클레임 사용
  ```

  * 이를 적용하면 PV와 PVC가 연결되고 해당 디스크에 포드가 마운트됨을 확인할 수 있다.
  * 이 과정을 정리하면 아래와 같다.
    1. 포드의 데이터를 영속적으로 저장하기 위해 클라우드 플랫폼에서 볼륨을 생성한다.
    2. YAML 파일로 생성한 볼륨을 쿠버네티스에서 퍼시스턴트 볼륨으로 등혹한다.
    3. YAML 파일로 퍼시트턴트 볼륨 클레임을 생성한다.
    4. 퍼시스턴트 볼륨과 퍼시트턴트 볼륨 클레임의 오구 조건이 일치하면 두 리소스가 연결된다.
    5. 포드에서 볼륨 클레임을 사용하도록 설정한다. 볼륨이 컨테이너 내부에 마운트된다.

* 퍼시스턴트 볼륨을 선택하기 위한 조건 명시

  * 애플리케이션에서 필요한 조건을 맞추기 위해 적어도 특정 조건을 만족하는 볼륨만을 사용해야 한다는 퍼시스턴트 볼륨 클레임을 쿠버네티스에게 알려 줄 수 있다.

  * `accessModes`는 볼륨에 대한 읽기, 쓰기 작업이 가능한지, 여러 개의 인스턴스에 의해 마운트될 수 있는지 등을 의미한다.

    | accessModes 이름 | kubectl get에서 출력되는 이름 | 속성 설명                         |
    | ---------------- | ----------------------------- | --------------------------------- |
    | ReadWriteOnce    | RWO                           | 1:1 마운트만 가능, 읽기 쓰기 가능 |
    | ReadOnlyMany     | ROX                           | 1:N 마운트 가능, 읽기 전용        |
    | ReadWriteMany    | RWX                           | 1:N 마운트 가능, 읽기 쓰기 가능   |

  * 스토리지 클래스나 라벨 셀렉터를 이용해 퍼시스턴트 볼륨의 선택을 세분화할 수도 있다.

* 퍼시스턴트 볼륨의 라이프사이클와 Reclaim Policy
  * `kubectl get pv` 명령어로 목록을 확인하면 `STATUS`라는 항목을 볼 수 있다. 이 항목은 퍼시스턴트 볼륨이 사용 가능한지, 볼륨 클레임과 연결됐는지 등을 의미한다. 퍼시스턴트 볼륨 클레임 연결 후 볼륨 클레임을 삭제하면 `Released` 상태로 변경되는데 이 상태의 퍼시스턴트 볼륨은 다시 사용할 수 없다. 그렇지만 실제 데이터는 볼륨 안에 보존돼 있으므로 볼륨을 삭제한 뒤 다시 생성하면 사용 가능하다.
  * 퍼시스턴트 볼륨 클레임을 퍼시스턴트 볼륨의 데이터를 어떻게 처리할 것인지 별도로 정의할 수 있다. 이를 Reclaim Policy라고 부른다.
    * Retain: 데이터를 보존한다. 이때 볼륨의 라이프사이클은 Available → Bound → Released가 된다.
    * Delete: 사용이 끝난 뒤에 자동으로 퍼시스턴트 볼륨이 삭제된다. 가능한 경우에 한해서 연결된 외부 스토리지도 삭제된다.
    * Recycle: 데이터를 삭제한 뒤 Available 상태로 만들어 준다. 퍼시스턴트 볼륨이나 외부 스토리지 자체를 삭제하지는 않지만 Deprecated된 기능이다.

###### 다이나믹 프로비저닝과 스토리지 클래스

* 볼륨 스토리지를 생성하고 스토리지에 대한 접근 정보를 YAML 파일에 적는 것은 번거로우므로 쿠버네티스는 다이나믹 프로비저닝이라는 기능을 제공한다. 이는 퍼시스턴트 볼륨 클레임이 요구하는 조건과 일치하는 퍼시스턴트 볼륨이 존재하지 않는다면 자동으로 퍼시스턴트 볼륨과 외부 스토리지를 함께 프로비저닝하는 기능이다.

* 스토리지 클래스는 특정 퍼시스턴트 볼륨을 선택하기 위해서뿐 아니라 다이나믹 프로비저닝에도 사용 가능하다. 이 정보를 참고해 외부 스토리지를 생성한다.

* 다이나믹 프로비저닝 기능이 지원되는 스토리지 프로비저너가 미리 활성화돼 있어야 한다.

* YAML 파일로 스토리지 클래스를 작성해 보자. (GKE, `pd-ssd`)

  ```yaml
  kind: StorageClass
  apiVersion: storage.k8s.io/v1
  metadata:
    name: fast
  provisioner: kubernetes.io/gce-pd
  parameters:
    type: pd-ssd
    zone: us-central1-c
  ```

  * `provisioner`에 클라우드 플랫폼과 쿠버네티스에서 사용할 수 있는 프로비저너를 설저한다.

  * `type` 항목은 Persistent disk의 종류를 나타낸다.

  * 이 스토리지 클래스를 사용하는 퍼시스턴트 볼륨 클레임을 생성함으로써 다이나믹 프로비저닝이 가능해진다. 

    > GKE에서는 기본적으로 사용할 수 있는 스토리지 클래스가 이미 존재하기 때문에 스토리지 클래스 생성 없이 `pd-standard` 타입으로 다이나믹 프로비저닝이 가능하다.

  ```yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: my-pvc-fast
  spec:
    storageClassName: "fast"        # fast 스토리지 클래스를 통해 다이나믹 프로비저닝
    accessModes:
      - ReadWriteOnce              
    resources:
      requests:
        storage: 5Gi                # 디스크 크기 
  ```

  * 다이나믹 프로비저닝을 사용할 때에는 Reclaim Policy가 자동으로 Delete로 설정되므로 스토리지 클래스를 정의할 때 다른 정책으로 설정하거나 `kubectl edit` 등의 명령어로 퍼시스턴트 볼륨의 속성을 직접 변경해 준다.

* 다이나믹 프로비저닝을 통해 PV 사용하도록 변경하기

  * 이전에 배포했던 서비스는 PV를 사용하지 않았으므로, 이를 변경해서 다이나믹 프로비저닝을 통해 PV를 사용하도록 한다. PVC를 작성하고 이를 MySQL 디플로이먼트 YAML 파일에서 사용하도록 변경한다. 기본적으로 사용할 수 있는 스토리지 클래스를 사용해서 PVC만 작성했다.

  ```yaml
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: urlshortener-mysql-pvc
  spec:
    storageClassName: "standard"
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 10Gi
  ```


  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    name: db
    labels:
      app: db
      tier: database
  spec:
    ports:
      - port: 3306
        targetPort: 3306
    selector:
      app: db
      tier: database
  ```

  ```yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: db
    labels:
      app: db
      tier: database
  spec:
    selector:
      matchLabels:
        app: db
        tier: database
    template:
      metadata:
        labels:
          app: db
          tier: database
      spec:
        containers:
        - image: mysql
          name: db
          env:
          - name: MYSQL_ROOT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: db-root-credentials
                key: password
          - name: MYSQL_DATABASE
            valueFrom:
              configMapKeyRef:
                name: db-conf
                key: name
          ports:
          - containerPort: 3306
            name: db
          volumeMounts:
          - name: db-persistent-storage
            mountPath: /var/lib/mysql
        volumes:
        - name: db-persistent-storage
          persistentVolumeClaim:
            claimName: urlshortener-mysql-pvc
  ```

  *  PV, PVC가 정상적으로 연결되고 포드도 정상적으로 생성되어 Running 상태, 즉 볼륨이 포드 내부에 정상적으로 마운트된 상태임을 확인할 수 있다.

  ![](https://user-images.githubusercontent.com/55083845/105953773-19056e00-60b7-11eb-96c9-5ce5e46c0c1b.png)

  * 다이나믹 프로비저닝으로 인해 Reclaim Policy가 Delete로 설정된 상태이므로 `kubectl patch pv <your-pv-name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'`를 통해 Retain으로 변경해 주었다.

##### 쿠버네티스 모니터링

* 실제 운영 단계의 클러스터의 경우 모니터링 시스템은 필수적이다. 사용자 요청이 갑작스럽게 몰려 부하가 발생할 때, 인프라 또는 애플리케이션에 장애가 발생했을 때 등에서 CPU나 메모리 같은 기초적인 모니터링 데이터를 확인하는 것부터 트러블슈팅을 시작할 수 있기 때문이다.

* 쿠버네티스는 자체적인 모니터링 기능을 제공하지는 않는다. 모니터링 시스템은 여러 오픈소스를 조합해서 구축하거나 클라우드 플랫폼 또는 상용 솔루션을 구매해서 사용한다.

* 가장 많이 사용되는 오픈소스 모니터링 데이터베이스는 프로메테우스(Prometheus)이다. 데이터베이스에 수집된 데이터를 그라파나(Grafana)라는 도구를 통해 시각화하거나 Alertmanager라고 불리는 도구를 통해 슬랙 알람을 보내는 등 여러 방법으로 사용할 수 있다. 

  * 오픈소스를 사용하는 경우 모든 관리를 직접 해야 한다는 단점이 있으므로 필요에 따라 유료 모니터링 솔류션을 사용할 수도 있다.

* 모니터링의 기본 구조

  * 모니터링 에이전트들은 특정 경로를 외부에 노출시켜 제공한다. 이 경로로 요청을 보내면 키-값 쌍으로 구성된 메트릭 데이터의 목록을 반환하는데, 이 메트릭 데이터를 프로메테우스 같은 시계열 데이터베이스에 저장한다. 모니터링 에이전트의 엔드포인트를 미리 지정해 주면 자동으로 데이터를 수집할 수 있다.

    > 이처럼 경로를 외부에 노출시켜 데이터를 수집할 수 있도록 인터페이스를 제공하는 서버를 exporter라고 부른다. 애플리케이션 내부의 데이터를 프로메테우스로 수집하고 싶다면 파이썬이나 Golang 등에서 제공하는 exporter 라이브러리를 통해 커스터마이징할 수 있다. 

* 모니터링 메트릭 분류

  * 인프라 수준의 메트릭: 호스트 레벨에서의 메트릭을 의미한다. 호스트에서 사용 중인 파일 디스크립터의 개수, 호스트에 마운트돼 있는 디스크 사용량, 호스트 NIC의 패킷 전송량 등이 될 수 있다.
  * 컨테이너 수준의 메트릭: 컨테이너 레벨에서의 메트릭을 의미한다. 컨테이너별 CPU와 메모리 사용량, 컨테이너 프로세스의 상태, 컨테이너에 할당된 리소스 할당량, 쿠버네티스 포드의 상태 등이 포함될 수 있다.
  * 애플리케이션 수준의 메트릭: 마이크로서비스에서 발생하는 트레이싱 데이터나 애플리케이션 로직에 종속적인 데이터일 수 있으며 서버 프레임워크에서 제공하는 모니터링 데이터베이스일 수도 있다.

* 쿠버네티스 모니터링 기초

  * 쿠버네티스는 메트릭을 수집해 사용할 수 있도록 몇 가지 애드온을 제공한다. 그중 가장 기초적인 것은 컨테이너와 인프라 레벨에서의 메트릭을 수집하는 `metrics-server`라는 도구이다. 이를 설치하면 포드의 오토스케일링, 사용 중인 리소스 확인 등 여러 기능을 추가적으로 사용할 수 있다. YAML 파일을 공식 깃허브 저장소에서 제공한다. 내려받고 디플로이먼트 실행 옵션(`args`)에 `--kubelet-insecure-tls` 옵션 추가 후 리소스를 생성한다.

  * `kubectl get po -n kube-system | grep metrics-server`로 생성을 확인할 수 있다. `kubectl top` 명령어를 통해 `metrics server`가 모아 온 클러스터 내부의 메트릭으로 리소스 사용량을 확인 가능하다.

    * 어떤 방식으로 동작하는 걸까?
    * 쿠버네시트의 노드 에이전트인 kubelet은 CAdvisor를 자체적으로 내장하고 있으며 포드와 노드 메트릭을 반환하는 `/stats/summary`라는 엔드포인트를 제공한다. kubelet은 기본적으로 노드의 10250 포트로 연결돼 있으므로 이 엔드포인트의 `/stats/summary`로 접근해 보면 Unauthorized 응답이 돌아온다. 이는 접근하기 위한 권한 정보를 넣어 주지 않았기 때문이다. 
    * 즉, kubelet으로부터 메트릭을 가져오기 위한 권한을 클러스터롤(`ClusterRole`)로 정의해서 서비스 어카운트에 연결한 뒤, 어카운트의 토큰을 curl 헤더에 포함시켜 요청해야 정상적인 메트릭을 확인할 수 있다. `metrics-server`의 YAML 파일에는 서비스 어카운트가 리소스에 접근할 수 있도록 권한이 이미 부여돼 있다.
    * `metrics-server`는 각 노드의 `/stats/summary`의 데이터를 수집해서 제공하는 것이다. `metrics-server`는 일종의 확장된 API 서버 역할을 한다. API 서버의 스펙을 준수하는 별도의 서버를 구축한 뒤, 이를 쿠버네티스에 등록해서 마치 확장된 API 서버처럼 사용할 수 있다. `metrics-server`는 쿠버네티스에 확장된 API 서버로 등록되어 있다. 확장된 API는 `APIService`라고 하는 리소스를 사용해 쿠버네티스에 등록할 수 있다. 이 리소스는 `metrics-server`를 배포할 때 함께 생성됐다.
    * `APIService`에는 새로운 API를 확장해 사용하기 위해서는 어떠한 서비스에 접근해야 하는지를 정의한다.  이러한 API 확장 방식을 쿠버네티스에서는 API Aggregation이라고 한다. `metrics-server`에서 사용하는 리소스는 아래와 같은 내용을 가진다.

    ```yaml
    apiVersion: apiregistration.k8s.io/v1
    kind: APIService
    metadata:
      labels:
        k8s-app: metrics-server
      name: v1beta1.metrics.k8s.io
    spec:
      group: metrics.k8s.io # 라는 이름의 API를 사용할 수 있다
      groupPriorityMinimum: 100
      insecureSkipTLSVerify: true
      service:
        name: metrics-server # 이 서비스 이름에 접근함으로써
        namespace: kube-system # 여기에 존재하는
      version: v1beta1 # API 버전
      versionPriority: 100
    ```

    1. `APIService` 리소스를 통해 `metrics-server`를 확장된 API 서버로 등록한다.
    2. 사용자는 쿠버네티스 API 서버(`kube-apiserver`에 `metrics.k8s.io` API 요청을 전송한다.
    3. 해당 API는 API Aggregation에 의해 `metrics-server`로부터 제공되고 있으므로 쿠버네티스 API는 해당 요청을 전달한다.
    4. 사용자는 `metrics-server`가 처리한 API 응답을 반환받는다. 

  * `kube-state-metrics`는 쿠버네티스 리소스의 상태에 관련된 메트릭을 제공하는 애드온이다. 설치를 위한 YAML 파일을 깃허브에서 제공하므로 쉽게 설치 가능하다. 구성이 단순하므로 `curl` 등으로 요처을 보내면 관련 메트릭을 바로 확인 가능하다.

  * `node-exporter`는 인프라 수준에서의 메트릭을 제공한다. 파일 시스템, 네트워크 패킷 등과 같이 호스트 측면에서의 다양하고 자세한 메트릭을 제공한다. `kube-prometheus`와 같은 깃허브 저장소에서 쿠버네티스용 `node-exporter` 매니페스토를 찾을 수 있다. YAML 파일을 받아 설치하고 나면 `kubectl get po -o wide -n monitoring`으로 확인 가능하다. 데몬셋으로 배포되므로 모든 노드에 포드가 하나씩 생성돼 있다.

###### 프로메테우스를 활용한 메트릭 수집

* 애드온 설치를 완료했다면 프로메테우스를 설치해 메트릭을 수집해 보자. 애드온들은 각자가 제공하는 메트릭을 외부로 수집할 수 있도록 엔드포인트를 제공한다. 이를 프로메테우스에 설정해 주면 메트릭이 자동적으로 수집된다.

* 프로메테우스 설치

  * 프로메테우스 도커 이미지를 사용하는 디플로이먼트를 배포하거나 프로메테우스 오퍼레이터와 커스텀 리소스를 활용해 프로메테우스를 관리하는 방법 등이 있다. 후자는 `prometheus`라는 커스텀 리소스를 생성하면 실제로 프로메테우스 포드가 생성된다고 생각하면 된다. 필요한 설정을 쉽게 적용할 수 있고, 커스텀 리소스를 사용하면 쉽게 사용할 수 있다. 프로메테우스 오퍼레이터는 깃허브 저장소에서 YAML 파일을 제공한다.

  ```bash
  git clone https://github.com/prometheus-operator/prometheus-operator.git
  cd prometheus-operator && kubectl apply -f bundle.yaml
  ```

  * `prometheus`라는 이름의 커스텀 리소스를 생성해 프로메테우스를 배포한다.

  ```yaml
  apiVersion: monitoring.coreos.com/v1
  kind: Prometheus
  metadata:
    name: prometheus
    namespace: default
  spec:
    replicas: 1
    serviceMonitorNamespaceSelector: {}
    serviceMonitorSelector: {}
  
  ```

  * 생성된 서비스는 `ClusterIP` 타입이므로 브라우저 접근이 불가하다. `kubectl port-forward svc/<service-name> local-port:service-port` 명령어를 사용해서 외부로 노출되지 않는 서비스에 임시로 접근할 수 있도록 로컬호스트에 포트 포워딩을 생성하고 브라우저로 접속하면 프로메테우스 웹 UI를 사용할 수 있다.

##### References

[시작하세요! 도커/쿠버네티스 (개정판)](https://wikibook.co.kr/docker-kube-rev/)