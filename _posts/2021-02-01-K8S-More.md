---
title: 쿠버네티스 좀 더 깊게 알아보기
categories:
  - Infra
tags:
  - k8s
toc: true
toc_label: "Contents"
toc_sticky: true
---

### 쿠버네티스 좀 더 깊게 알아보기; 고급 기능 활용하기

#### 인그레스(ingress)?

* 인그레스란 일반적으로 외부에서 내부로 향하는 것을 지칭하는 단어로, 인그레스 트래픽은 외부에서 서버로 유입되는 트래픽을 의미하고, 인그레스 네트워크는 인그레스 트래픽을 처리하기 위한 네트워크를 의미한다.

* 서비스 오브젝트가 외부 요청을 받아들이기 위한 것이었다면 **인그레스**는 외부 요청을 어떻게 처리할 것인지 네트워크 7계층 레벨에서 정의하는 쿠버네티스 오브젝트이다.
  * 외부 요청의 라우팅: 특정 경로로 들어온 요청을 어떠한 서비스로 전달할지 정의하는 라우팅 규칙 설정
  * 가상 호스트 기반의 요청 처리: 같은 IP에 대해 다른 도메인 이름으로 요청이 도착했을 때 어떻게 처리할 것인지 정의
  * SSL/TLS 보안 연결 처리: 여러 개의 서비스로 요청을 라우팅할 때 보안 연결을 위한 인증서 적용
  * etc...

##### 왜 인그레스를 사용할까?

* 여러 개의 드플로이먼트가 존재할 때 인그레스가 없다면 각 디플로이먼트를 외부에 노출하기 위해 서비스를 여러 개 생성하는 방법이 있을 것이다. 각 디플로이먼트에 대응하는 서비스를 하나씩 연결해 줄 때 서비스마다 세부적인 설정 시 추가적인 복잡성이 발생하게 된다.
* 인그레스 오브젝트를 사용하면 URL 엔드포인트를 단 하나만 생성함으로써 이러한 복잡성을 쉽게 해결할 수 있다. 클라이언트는 인그레스의 URL로만 접근하게 되며 해당 요청은 인그레스에서 정의한 규칙에 따라 처리된 뒤 적절한 디플로이먼트의 포드로 전달된다.
  * 이때 라우팅 정의가 보안 연결 등과 같은 설정은 인그레스에 의해 수행된다. 즉, 외부 요청에 대한 처리 규칙을 쿠버네티스 자체의 기능으로 편리하게 관리 가능하다.

##### 인그레스의 구조

* `ingress`라는 이름으로 사용 가능하다.

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: ingress-example
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: "nginx"
spec:
  rules:
  - host: sshyeonzz.example.com # 요청 보낼 때 해당 도메인인지 주의!
    http:
      paths:
      - path: /echo-hostname
        backend:
          serviceName: hostname-service
          servicePort: 80
```

* `host`: 해당 도메인 이름으로 접근하는 요청에 대해서 처리 규칙을 적용한다. 여러 개의 `host`를 정의해 사용 가능하다.
* `path`: 해당 경로에 들어온 요청을 어느 서비스로 전달할 것인지 정의한다. 위 예시에서는 `/echo-hostname`이라는 경로의 요청을 `backend`에 정의된 서비스로 전달한다. 여러 개 정의 가능하다.
* `serviceName`, `servicePort`: `path`로 들어온 요청이 전달될 서비스와 포트이다.

* 인그레스를 생성하는 것만으로는 아무일도 일어나지 않는다. 인그레스는 요청을 처리하는 규칙을 정의할 뿐 외부 요청을 받아들일 수 있는 실제 서버가 아니다.
  * 인그레스는 **인그레스 컨트롤러(Ingress Controller)**라는 특수한 서버에 적용해야 규칙을 사용할 수 있다.
  * 실제로 외부 요청을 받아들이는 것은 인그레스 컨트롤러 서버이고, 이 서버가 인그레스 규칙을 로드해 사용한다.
  * 따라서 인그레스는 반드시 인그레스 컨트롤러라는 서버와 함께 사용해야 한다. 여러 종류가 있으며 필요에 따라 하나를 골라서 사용한다.
* 대표적으로 Nginx 웹 서버 인그레스 컨트롤러, Kong, GKE 등의 클라우스 플랫폼에서 제공되는 인그레스 컨트롤러가 존재한다. Nginx 인그레스 컨트롤러는 쿠버네티스에서 공식적으로 개발되고 있다.
  * YAML 파일을 공식 깃허브에서 받아 사용할 수 있다. `LoadBalancer` 타입으로 서비스가 생성된다.
  * 클라우드가 아닌 환경에서 테스트하고 싶은 경우 `NodePort` 타입의 서비스를 생성해 사용할 수 있다. 이 경우 각 노드의 랜덤한 포트로 인그레스 컨트롤러에 접근할 수 있다.
  * 인그레스 컨트롤러에 의해 요청이 최종적으로 도착할 디플로이먼트의 서비스는 굳이 외부에 노출할 필요가 없는 경우 `ClusterIP` 타입이 좋다.
  * 도메인 이름이 다를 경우 요청을 처리하지 않으므로 다른 경우  `curl` 명령어의 `--resolve` 옵션으로 임시로 도메인 명을 설정하거나 `/etc/hosts` 파일에 IP와 도메인을 설정해 임시로 동작하게 하는 등의 조치를 취한다. 혹은 인그레스 YAML 파일을 수정한다.
* 전체 구조는 아래 그림과 같다.

![](https://www.nginx.com/wp-content/uploads/2018/12/NGINX-Ingress-Controller-4-services_social.png)

* 인그레스를 사용하는 순서는 아래와 같다.

  1. 공식 깃허브에서 제공되는 YAML 파일로 Nginx 인그레스 컨트롤러를 생성한다.
  2. Nginx 인그레스 컨트롤러를 외부로 노출하기 위한 서비스를 생성한다.
  3. 요청 처리 규칙을 정의하는 인그레스 오브젝트를 생성한다.
  4. Nginx 인그레스 컨트롤러로 들어온 요청은 인그레스 규칙에 따라 적절한 서비스로 전달된다.

  * 인그레스를 생성하면 인그레스 컨트롤러는 자동으로 인그레스를 로드해 적용한다.
  * 특정 경로와 호스트 이름으로 들어온 요청은 인그레스에 정의된 규칙에 따라 서비스로 전달된다. 요청이 실제로 서비스로 전달되는 것은 아니고, Nginx 인그레스 컨트롤러는 서비스에 의해 생성된 엔드포인트로 요청을 *직접* 전달한다.
  * 이러한 동작을 쿠버네티스에서는 바이패스(bypass)라고 부른다. 서비스를 거치지 않고 포드로 직접 요청이 전달되기 때문이다.

##### annotation으로 인그레스 설정하기

```yaml
...
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    kubernetes.io/ingress.class: "nginx"
...
```

* `kubernetes.io/ingress.class`는 해당 인그레스 규칙을 어떤 인그레스 컨트롤러에 적용할 것인지를 나타낸다. 인그레스 컨트롤러 서버는 여러 가지 중 하나를 선택해 사용할 수 있지만 쿠버네티스 클러스터 자체에서 기본적으로 사용하도록 설정된 것이 존재할 수 있다. 이 경우 어떤 인그레스 컨트롤러를 사용할 것인지 명시해 주는 게 좋다.

* `nginx.ingress.kubernetes.io.rewrite-target`은 Nginx 인그레스 컨트롤러에서만 사용할 수 있는 기능으로 인그레스에 정의된 경로로 들어오는 요청을 `rewrite-target`에 설정된 경로로 전달한다.

  * Nginx의 캡처 그룹과 함께 사용될 때 유용한 기능이다.
  * 캡처 그룹이란 *정규 표현식의 형태로 요청 경로 등의 값을 변수로서 사용*할 수 있는 방법이다.

  ```yaml
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  metadata:
    name: ingress-example
    annotations:
      nginx.ingress.kubernetes.io/rewrite-target: /$2 # path의 (.*)에서 획득한 경로로 전달
      kubernetes.io/ingress.class: "nginx"
  spec:
    rules:
    - host: sshyeonzz.example.com # 요청 보낼 때 해당 도메인인지 주의!
      http:
        paths:
        - path: /echo-hostname(/$)(.*) # (.*)를 통해 경로 획득
          backend:
            serviceName: hostname-service
            servicePort: 80
  ```

  * 그 외에 다양한 주석이 존재한다. 인그레스 컨트롤러의 공식 문서를 참고해 사용하자.

##### Nginx 인그레스 컨트롤러에 SSL/TLS 보안 연결 적용

* 클라우드 환경에서 `LoadBalancer` 타입의 서비스를 사용한다면 크랄우드 플랫폼 자체에서 관리해 주는 인증서를 인그레스 컨트롤러에 적용 가능하다.
* 직접 서명한 루트 인증서를 통해 Nginx 인그레스 컨트롤러에 적용해 보자.

```bash
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
-keyout tls.key -out tls.crt -subj "/CN=<public DNS name>"
```

* 인그레스 설정에 TLS 옵션 추가하기

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: ingress-example
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    kubernetes.io/ingress.class: "nginx"
spec:
  tls:
  - hosts:
    - sshyeonzz.example.com
    secretName: tls-secret
  rules:
  - host: sshyeonzz.example.com # 요청 보낼 때 해당 도메인인지 주의!
    http:
      paths:
      - path: /echo-hostname(/$)(.*)
        backend:
          serviceName: hostname-service
          servicePort: 80
```

##### 여러 개의 인그레스 컨트롤러 사용하기

* Nginx 인그레스 컨트롤러는 기본적으로 nginx라는 이름의 클래스를 가지고 있으며, 이 설정을 변경함으로써 여러 개의 Nginx 인그레스 컨트롤러를 사용할 수 있고, 인그레스 규칙을 선택적으로 적용할 수도 있다.

* 별도의 값(클래스)을 설정한 인그레스 컨트롤러를 생성해 `kubernetes.io/ingress.class`를 해당 값으로 설정할 수 있다.
* 이 외에도 주석 접두어 설정 옵션이나 설정을 저장하는 컨피그맵을 지정하는 옵션 등을 수정해서 원하는 구성을 완성할 수 있다.

#### 보안을 위한 인증과 인가: ServiceAccount와 RBAC

* 쿠버네티스는 보안 측면에서 다양한 기능을 제공하고 있는데 그중에서 가장 자주 사용되는 것은 RBAC(Role Based Access Control)를 기반으로 하는 서비스 어카운트라는 기능이다. 서비스 어카운트는 사용자 또는 애플리케이션 하나에 해당하며, RBAC라는 기능을 통해 특정 명령을 실행할 수 있는 권한을 서비스 어카운트에 부여한다. 권한을 부여받은 서비스 어카운트는 해당 권한에 해당하는 기능만 사용할 수 있게 된다.

##### 쿠버네티스의 권한 인증 과정

* `kubectl` 명령어로 쿠버네티스 기능을 실행하면 쿠버네티스 내부에서는 아래와 같은 절차를 거쳐 기능을 실행한다.
  1. 쿠버네티스 API 서버의 HTTP 핸들러에 요청 전송
  2. API 서버는 해당 클라이언트가 쿠버네티스의 사용자가 맞는지(Authentication: 인증), 해당 기능을 실행할 권한이 있는지(Authorization: 인가) 확인한다. 인증과 인가에는 서비스 어카운트 외에 서드파티 인증(Open ID Connect: OAuth), 인증서 등과 같이 다양한 방법 사용 가능
  3. 어드미션 컨트롤러(Admission Controller)라는 별도의 단계를 거친 뒤 요청받은 기능 수행
* 설치 도구를 이용해 쿠버네티스 설치 시 설치 도구가 자동으로 `kubectl`이 관리자 권한을 갖도록 설정한다. 이 설정은 `~/kube/config`라는 파일에서 확인 가능하다.
  * `kubectl`을 사용할 때는 기본적으로 이 파일에 저장된 설정을 읽어 쿠버네티스 클러스터를 제어한다. 이 파일에 저장된 내용 중에서 `users`라는 항목에는 인증을 위한 데이터가 설정돼 있다. `cluster`라는 항목에는 클러스터에 접근하기 위한 정보를 저장한다.
  * 이 파일에서 은증서 키 쌍을 사용해 API 서버에 인증하지만 이 방식은 절차가 복잡하고 관리가 어려워 자주 사용하지는 않는다. 
  * 쿠버네티스에서는 인증을 위해 인증서 키 쌍뿐 아니라 여러 가지 방법을 사용할 수 있고 그중 하나가 *서비스 어카운트*이다.

##### 서비스 어카운트와 롤(Role), 클러스터 롤(Cluster Role)

* 서비스 어카운트는 체계적으로 권한을 관리하기 위한 쿠버네티스 오브젝트이다. 네임스페이스에 속하는 오브젝트로 `serviceaccount(sa)`라는 이름으로 사용 가능하다. `kubectl create sa`로 생성할 수 있다.
  * 임시로 특정 서비스 어카운트를 사용하기 위해 `--as` 옵션을 사용 가능하다. `kubectl get services --as system:serviceaccount:default:<serviceaccount-name>`에서 `system:serviceaccount`는 인증을 위해 서비스 어카운트를 사용한다는 것을 나타내며, `default:<serviceaccount-name>`은 `default` 네임스페이스의 특정 어카운트를 나타낸다.
* 서비스 어카운트를 생성하고 나면 적절한 권한을 부여해 주어야 기능을 사용할 수 있다. 쿠버네티스에서 권한을 부여하는 방법에는 크게 두 가지가 있다. 롤(Role)과 클러스터 롤(Cluster Role)을 이용해 권한을 설정한다. 
  * 롤과 클러스터 롤은 부여할 권한이 무엇인지를 나타내는 쿠버네티스 오브젝트이다. 롤은 네임스페이스에 속하는 오브젝트이므로 디플로이먼트나 서비스처럼 네임스페이스에 속하는 오브젝트들에 대한 권한을 정의할 때 쓰인다.
  * 클러스터 롤은 클러스터 단위의 권한을 정의할 때 사용한다. 네임스페이스에 속하지 않는 오브젝트뿐만 아니라 클러스터 전반에 걸친 기능을 사용하기 위해서 정의할 수 있고, 여러 네임스페이스에서 반복적으로 사용되는 권한을 클러스터 롤로 만들어 재사용하는 것도 가능하다.
* 롤을 생성해 본다.

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default # 롤이 생성될 네임스페이스
  name: service-reader # 롤의 이름
roles:
- apiGroups: [""] 
  resources: ["services"] 
  verbs: ["get", "list"]
```

* **`rules`**의 내용에 주목해 보자.

  * `apiGroups`:어떠한 API 그룹에 속하는 오브젝트에 대해 권한을 지정할지 설정한다. API 그룹은 쿠버네티스의 오브젝트가 가지는 목적에 따라 분류되는 일종의 카테고리이다. `""`는 포드, 서비스 등이 포함된 코어 API 그룹을 의미한다. `kubectl api-resources` 명령어를 사용하면 특정 쿠버네티스 오브젝트가 어떤 API 그룹에 속하는지 확인할 수 있다.
  * `resources`: 어떠한 쿠버네티스 오브젝트에 대해 권한을 정의할 것인지 입력한다.
  * `verbs`: 이 롤을 부여받은 대상이 `resources`에 저장된 오브젝트들에 대해 어떤 동작을 수행할 수 있는지 정의한다. 예시에서는 `kubectl get services` 명령어로 개별 서비스의 정보를 가져오거나 모든 서비스 목록을 확인할 수 있는 권한이 부여된다.
  * 롤은 특정 기능에 대한 권한을 정의하는 오브젝트이므로 이 롤을 특정 대상에게 부여하려면 롤 바인딩이라는 오브젝트를 통해 특정 대상과 롤을 연결해야 한다.

  ```yaml
  apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    name: service-reader-rolebinding
    namespace: default
  subjects:
  - kind: ServiceAccount # 권한을 부여할 대상이 ServiceAccount
    name: <serviceaccount-name> # 권한을 부여할 서비스 어카운트 이름
    namespace: default 
  roleRef:
    kind: Role # Role에 정의된 권한 부여
    name: <role-name> # 기재된 이름의 롤을 대상(subjects)에 연결
    apiGroup: rbac.authorization.k8s.io
  ```

  * 여기까지 작업을 마치면 (권한을 부여받았으므로) 서비스 어카운트를 사용해서 명령어를 실행할 수 있다. 권한이 부여되지 않은 다른 기능은 사용할 수 없다.

  ![](https://user-images.githubusercontent.com/55083845/106421852-d53cab00-64a0-11eb-9f11-594c24be0687.png)

  * 롤 바인딩과 롤, 서비스 어카운트는 모두 1:1 관계가 아니다. 하나의 롤은 여러 개의 롤 바인딩에 의해 참조될 수 있고, 하나의 서비스 어카운트는 여러 개의 롤 바인딩에 의해 구너한을 부여받을 수 있다. 롤은 권한을 부여받기 위한 일종의 템플릿과 같은 역할을 하고, 롤 바인딩은 롤과 서비스 어카운트를 연결하기 위한 중간 다리 역할을 한다.

###### 롤과 클러스터 롤

* 롤과 롤 바인딩은 네임스페이스에 한정되는 오브젝트로 포드, 서비스, 디플로이먼트 등과 같이 네임스페이스에 한정된 오브젝트에 대한 권한을 정의하기 위해 사용할 수 있다. 
* 네임스페이스에 종속되지 않는 오브젝트도 존재한다. 클러스터 수준의 오브젝트들에 대한 접근 권한은 서비스 어카운트에 기본적으로 설정돼 있지 않다. 클러스터 수준의 오브젝트, 모든 네임스페이스의 리소스에 대한 동작을 위해서는 롤 대신 클러스터 롤을 사용할 수 있다. 클러스터 롤은 클러스터 단위의 리소스에 대한 권한을 정의하기 위해 사용한다.

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  namespace: default
  name: nodes-reader
roles:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list"]
```

* 롤과 크게 다르지 않다. 롤을 사용할 때와 마찬가지로 클러스터 롤을 특정 대상에게 연결하려면 클러스터 롤 바인딩이라는 오브젝트를 사용해야 한다.

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: nodes-reader-rolebinding
  namespace: default
subjects:
- kind: ServiceAccount # 권한을 부여할 대상이 ServiceAccount
  name: <serviceaccount-name> # 권한을 부여할 서비스 어카운트 이름
  namespace: default 
roleRef:
  kind: ClusterRole # Role에 정의된 권한 부여
  name: <role-name> # 기재된 이름의 롤을 대상(subjects)에 연결
  apiGroup: rbac.authorization.k8s.io
```

###### 여러 개의 클러스터 롤 조합해서 사용하기

* 자주 사용되는 클러스터 롤의 경우 다른 클러스터 롤에 포함시켜 재사용할 수 있다. 이를 클러스터 롤 애그리게이션(aggregation)이라고 한다.

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: parent-clusterrole
  labels:
    rbac.authorization.k8s.io/aggregation-to-child-clusterrole: "true"
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: child-clusterrole
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
    rbac.authorization.k8s.io/aggregate-to-child-clusterrole: "true"
rules: []
```

* 클러스터 롤에 포함시키고자 하는 다른 클러스터 롤을 `aggregationRule.clusterRoleSelectors` 아래의 `matchLabels`의 라벨 셀렉터로 선택하면 하위 클러스터 롤에 포함돼 있는 권한을 그대로 부여받을 수 있다.
* 이 기능을 활용하면 여러 개의 클러스터 롤 권한을 하나의 클러스터 롤에 합쳐서 사용하거나 여러 단계의 상속 구조를 만들 수 있다.

##### 쿠버네티스 API 서버에 접근하기

* 애플리케이션이 쿠버네티스 API를 사용해야 한다면 일반적으로 `kubectl`이 아닌 다른 방법으로 API 서버에 접근할 것이다.

* 쿠버네티스의 API 서버는 HTTP 요청을 통해 쿠버네티스의 기능을 사용할 수 있도록 REST API를 제공한다. 쿠버네티스의 REST API에 접근하기 위한 엔드포인트는 자동으로 개방되므로 별도의 설정 없이 API 서버에 접근할 수 있다.

  * kubeadm의 경우 쿠버네티스의 마스터 IP와 6443 포트로, GKE나 kops의 경우 443 포트로 접근하면 API 서버에 연결할 수 있다. 마스터 노드에 SSH로 직접 접속할 수 있다면 SSH로 접속한 뒤 localhost로 요청을 보내도 되지만 원격에서 마스터 서버에 접근하고 싶다면 `~/kube/config` 파일에서 `server` 항목을 찾아 해당 주소로 요청을 보내도 된다.
  * 단, 쿠버네티스 API는 기본적으로 HTTPS 요청만 처리하도록 되어 있으며 기본적으로 보안 연결을 위해 스스로 사인한 인증서를 사용한다.
  * API 서버에 접근하기 위해서는 별도의 인증 정보를 HTTP 페이로드에 포함시켜 REST API 요청을 전송해야 한다. 이를 위해 쿠버네티스는 서비스 어카운트를 위한 인증 정보를 시크릿에 저장한다. 서비스 어카운트를 생성하면 이에 대응하는 시크릿이 자동으로 생성되며, 해당 시크릿은 서비스 어카운트를 증명하기 위한 수단으로 사용된다.
    * 서비스 어카운트와 연결된 시크릿에는 ca.crt, namespace, token 데이터가 저장돼 있다. ca.crt는 쿠버네티스 클러스터의 공개 인증서를, namespace는 해당 서비스 어카운트가 존재하는 네임스페이스를 저장한다. token 데이터는 쿠버네티스 API 서버와의 JWT 인증에 사용된다. 따라서 API 서버의 REST API 엔드포인트로 요청을 보낼 때 token 데이터를 함께 담아서 보내면 서비스 어카운트를 인증할 수 있다.
  * `kubectl`에서 사용할 수 있는 기능은 모두 REST API에서도 동일하게 사용 가능하다. 단, 요청을 위해서 롤 또는 클러스터 롤을 통해 서비스 어카운트에 권한을 부여해야 한다.

* 쿠버네티스 클러스터 내부에서 API 서버에 접근하는 건 어떻게 가능할까?

  * 포드 내부에서도 알림 등의 설정을 위해 쿠버네티스 API 서버에 접근하기 위한 방법이 필요하고, 권한 인증도 수행 가능해야 한다.
  * 쿠버네티스에서는 클러스터 내부에서 API 서버에 접근할 수 있는 서비스 리소스를 미리 생성해 둔다. `kubernetes`라는 이름의 서비스이다. 클러스터 내부에서 실행 중인 포드는 `default` 네임스페이스에서 `kubernetes` 서비스를 통해 API 서버에 접근할 수 있다.
    * 서비스 어카운트에 부여되는 시크릿의 토큰을 HTTP 요청에 담아 전달해야 인증과 인가가 가능한데, 쿠버네티스는 포드를 생성할 때 **자동으로 서비스 어카운트의 시크릿을 포드 내부에 마운트**한다. 따라서 포드 내부에서 API 서버에 접근하기 위해 시크릿의 데이터를 포드 내부로 가져올 필요는 없다. kubectl describe pods <pod-name>`으로 시크릿의 경로를 알 수 있다. 필요한 경우 해당 경로에서 내용을 읽어 사용하면 된다.
    * 포드를 생성하는 YAML 파일에 설정을 하지 않으면 자동으로 default 서비스 어카운트의 시크릿을 마운트하지만 `serviceAccountName` 항목을 YAML 파일에서 별도로 지정하면 특정 서비스 어카운트의 시크릿을 마운트할 수 있다.

* 포드 내부에서 실행되는 애플리케이션이라면 특정 언어로 바인딩된 쿠버네티스 SDK를 활용하는 프로그래밍 방식을 더 많이 사용할 것이다. 서비스 어카운트의 시크릿과 쿠버네티스 SDK를 이용해 API 서버에 접근하는 흐름은 아래와 같다.

  1. 롤과 롤 바인딩을 통해서 특정 서비스 어카운트에 권한이 부여돼 있어야 한다.
  2. YAML 파일에 `serviceAccountName` 항목을 명시적으로 지정해 포드를 생성한다.

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: k8s-python-sdk
  spec:
    serviceAccountName: sshyeonzz
    containers:
    - name: k8s-python-sdk
      image: alicek106/k8s-sdk-python:latest
  ```

  3. 포드 내부에 마운트된 시크릿을 확인한다.
  4. 포드 내부에서 쿠버네티스의 API를 사용할 수 있는 간단한 파이썬 코드를 작성한다.

  ```python
  from kubernetes import client, config
  
  config.load_incluster_config() # 포드 내부에 마운트된 서비스 어카운트의 토큰과 인증서 파일을 읽어 인증 및 인가 작업을 수행
  
  try:
  		print("Trying to list service...")
    	result = client.CoreV1Api().list_namespaced_service(namespace='default') # CoreV1 그룹의 API를 이용해 특정 네임스페이스의 서비스 목록을 출력한다
    	for item in result:
    			print('-> {}'.format(item.metadata.name))
  except client.rest.ApiException as e:
    	print(e)
    
  print('----')
  
  try:
  		print('Trying to list pod..')
    	result = client.CoreV1Api.list_namespaced_pod(namespace='default') # CoreV1 그룹의 API를 이용해 특정 네임스페이스의 포드 목록을 출력한다
    	for item in result:
    			print(item.metadata.name)
  except client.rest.ApiException as e:
  		print(e)
  ```

##### 서비스 어카운트에 이미지 레지스트리 접근을 위한 시크릿 설정

* 서바비스 어카운트를 이용하면 비공개 레지스트리 접근을 위한 시크릿을 서비스 어카운트 자체에 설정할 수 있다. 예를 들어, 다음과 같이 YAML 파일을 작성할 수 있다.

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: reg-auth
  namespace: default
imagePullSecrets:
- name: <secret-name>
```

##### kubeconfig 파일에 서비스 어카운트 인증 정보 설정

* `kubectl` 명령어를 사용해 쿠버네티스 클러스터를 제어할 때는 `kubeconfig`라고 하는 특수한 설정 파일을 통해 인증을 진행한다. 하지만 여러 명의 개발자가 `kubectl` 명령어를 사용해야 한다면 서비스 어카운트를 통해 적절한 권한을 조절하는 것이 나을 것이다.
  * 이를 위해 권한이 제한된 서비스 어카운트를 통해 `kubectl` 명령어를 사용하도록 `kubeconfig`에서 설정할 수 있다. 즉, 서비스 어카운트와 연결된 시크릿의 token 데이터를 `kubeconfig`에 명시함으로써 명령어의 권한을 제한할 수 있다.
  * `kubeconfig` 파일은 일반적으로 `~/.kube/config` 경로에 있으며 필요에 따라 `KUBECONFIG` 셸 환경 변수로 경로를 직접 설정할 수 있다. `kubectl`은 기본적으로 `kubeconfig` 설정 정보에서 API 서버의 주소와 사용자 인증 정보를 로드한다. `kubeconfig`는 크게 세 가지 파트로 나뉜다.
    * `clusters`: `kubectl`이 사용할 쿠버네티스 API 서버의 접속 정보 목록이다. 
    * `users`: 쿠버네티스의 API 서버에 접속하기 위한 사용자 인증 정보 목록이다. 서비스 어카운트의 토큰을 입력할 수도 있고, 쿠버네티스 클러스터에서 사용되는 루트 인증서에서 발급한 하위 인증서의 데이터를 입력할 수도 있다.
    * `contexts`: 위의 두 항목에 정의된 값을 조합해 최종적으로 사용할 쿠버네티스 클러스터의 정보를 설정한다. 여러 개의 사용자 인증 정보와 클러스터를 조합해 여러 개의 컨텍스트를 정의할 수 있다. 현재 사용하는 컨텍스트는 `kubeconfig` 파일의 `current-context` 항목에서 확인할 수 있다.
    * `kubeconfig`는 `~/.kube/config`에서 편집기로 수정할 수도 있지만 `kubectl config` 명령어를 사용해 수정할 수 있다. 
      * `kubectl config set-credentials <user-name> --token=<token-value>`: 사용자를 새롭게 등록한다.
      * `kubectl config set-context <context-name> --cluster=<cluster-name> --user=<user-name>`: 새 컨텍스트를 등록한다. `kubectl config get-cluster`로 클러스터를 확인할 수 있다.
      * `kubectl config use-context <context-name>`: 새로운 컨텍스트를 사용한다.

##### 유저(User)와 그룹(Group)

* 유저는 실제 사용자, 그룹은 여러 유저들을 모아 놓은 집합을 의미한다. (클러스터) 롤 바인딩 YAML 파일의 `kind` 값에는 `ServiceAccount` 대신 `User`나 `Group`을 사용할 수도 있다. 서비스 어카운트 또한 개념상으로 유저의 한 종류이지만 '사람'이라는 객체는 유저라는 개념으로 추상화해 사용되기 때문에 쿠버네티스 내부에서 사용자는 최종적으로 '유저'라는 개념으로 취급된다.
  * 쿠버네티스에는 유저나 그룹이라는 오브젝트가 없어 `kubectl get user/group` 명령어는 사용 불가하다. 
  * 롤 바인딩 생성 시 `kind`에 `ServiceAccount` 대신 `User`를 사용할 수 있다.
  * 그룹은 유저를 모아 놓은 집합으로 쿠버네티스에서 사용할 수 있는 대표적인 그룹은 서비스 어카운트의 집합인 `system:serviceaccounts`로 시작하는 그룹이다. 이 그룹은 모든 네임스페이스에 속하는 모든 서비스 어카운트가 속해 있는 그룹이다. 이외에도 다양한 그룹이 있으며 쿠버네티스에서 미리 정의된 유저나 그룹은 접두어 `system`을 사용한다.
* 쿠버네티스에서 별도의 인증 서버를 사용할 때에 유저와 그룹의 개념을 활용할 수 있다.

##### 커스텀 리소스와 컨트롤러

* 쿠버네티스는 포드와 같이 기본적인 리소스 외에도 직접 리소스의 종류를 정의해 사용할 수도 있는데 이를 커스텀 리소스라고 부른다. 커스텀 리소스를 제대로 사용하기 위해서는 컨트롤러라고 하는 별도의 컴포넌트를 이해하고 구현할 수 있어야 한다.

##### 쿠버네티스 컨트롤러의 개념과 동작 방식

* `docker run`처럼 특정 명령을 처리하는 주체와 통신해 그 작업을 수행하고 결과를 돌려받는 방식을 **명령형**이라고 한다.
* 쿠버네티스에는 이와 반대되는 **선언형** 방식을 지향한다. 선언형 방식은 최종적으로 도달해야 하는 바람직한 상태를 정의한 뒤 현재 상태가 바람직한 상태와 다른 경우 이를 일치하도록 만드는 방법이다. 대표적인 예시로 `kubectl apply -f` 명령어가 있다.
  * 뒤에 따라오는 YAML 파일은 최종적으로 도달해야 하는 상태를 의미하며, 쿠버네티스는 현재 상태가 해당 YAML 파일과 일치하도록 하는 특정 동작을 수행한다.
  * 명령형 방식의 대표적인 예는 `kubectl create -f`가 있다. 이 명령어는 구체적인 동작을 의미하고 쿠버네티스가 해당 동작을 수행하도록 명령한다.
  * 하지만 선언형 방식은 이와 다르다. `kubectl apply -f`는 특정 YAML 파일이 최종적으로 완성되어야 하는 상태라는 것을 알려 줄 뿐, 실제로 어떤 동작을 취해야 하는지는 명시하지 않는다. **최종적으로 완성되어야 하는 상태가 되기 위해 어떤 동작을 취할지는 쿠버네티스에서 컨트롤러라고 불리는 개체가 내부적으로 결정**한다.
  * 이런 이유로 쿠버네티스에는 유독 바람직한 상태(Desired State)라는 단어가 자주 등장한다. 해당 상태가 아니라면 해당 상태를 유지하기 위해 어떠한 동작을 하게 될 것이다. 대부분의 쿠버네티스 오브젝트는 이러한 원리에 따라 제어된다. 
  * 이론적으로 쿠버네티스의 컨트롤러는 모두 개별적으로 존재할 수 있으나 전체 구성의 복잡성을 줄이기 위해 컨트롤러 로직을 **쿠버네티스 컨트롤러 매니저**라는 하나의 컴포넌트에 구현해 놓았다. 디플로이먼트 컨트롤러, 노드 컨트롤러 등 다양한 컨트롤러가 동시에 실행된다.

##### 커스텀 리소스?

* 커스텀 리소스는 말 그대로 직접 정의해 사용할 수 있는 사용자 정의 리소스이다.  커스텀 리소스 또한 포드, 디플로이먼트 등과 동일한 리소스의 한 종류로 취급된다.
* 커스텀 리소스는 오브젝트의 묶음을 커스텀 리소서로 추상화함으로써 쿠버네티스 리소스를 묶어 놓은 패키지처럼 사용할 수도 있고, 쿠버네티스와 전혀 상관 없는 로직을 커스텀 리소스와 연동할 수도 있다.
  * 여러 리소스를 한꺼번에 생성해서 생애 주기를 쉽게 관리할 수 있다.
* 어떻게 사용할까?
  1. 현재 상태를 커스텀 리소스에 대한 바람직한 상태로 변화시킬 수 있는 컨트롤러를 구현하고 컨트롤러를 실행한다.
  2. 커스텀 리소스의 상세 정보를 정의하는 CRD(Custom Resource Definition) 리소스를 생성한다.
  3. CRD에 정의된 데이터에 맞춰 커스텀 리소스를 생성한다.
  4. 1번에서 생성한 컨트롤러는 커스텀 리소스의 생성을 감지하고 커스텀 리소스가 원하는 바람직한 상태가 되도록 적절한 작업을 수행한다.

##### 커스텀 리소스를 정의하기 위한 CRD(Custom Resource Definition)

* 커스텀 리소스는 CRD 오브젝트를 통해 정의할 수 있다. 커스텀 리소스를 어떻게 사용할 것인지 쿠버네티스에 등록하는 선언적인 리소스일 뿐, 그 자체가 커스텀 리소스를 의미하는 것은 아니다. YAML 파일을 통해 생성한다.

```yaml
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: sshyeonzz.k106.com # 1. CRD의 이름
spec:
  group: k106.com # 2. 커스텀 리소스의 API 그룹
  version: v1alpha1 # 커스텀 리소스의 API 버전
  scope: Namespaced # 커스텀 리소스가 네임스페이스에 속하는지 여부
  names:
    plural: sshyeonzzes # 3. 커스텀 리소스의 이름
    singular: sshyeonzz 
    kind: Sshyeonzz # YAML 파일 등에서 사용될 커스텀 리소스의 Kind
    shortNames: ["sz"] # 커스텀 리소스의 줄임말
  validation:
    openAPIV3Schema # 4. 커스텀 리소스의 데이터를 정의
    required: ["spec"] # 커스텀 리소스에는 반드시 "spec"이 존재해야 함
    properties: # 커스텀 리소스에 저장될 데이터 형식 정의
      spec:
        required: ["myvalue"]
        properties:
          myvalue:
            type: "string"
            minimum: 1
```

* `metadata.name`: CRD의 이름
* `spec.group`, `spec.version`: CRD를 통해 생성될 커스텀 리소스가 속할 API 그룹과 버전(`apiVersion: ~`)
* `spec.names`: 커스텀 리소스를 지칭할 이름
* `spec.validation`: 실제로 커스텀 리소스에 어떠한 데이터가 저장돼야 하며, 어떠한 항목이 반드시 설정되어야 하는지 정의한다. 예시는 커스텀 리소스에 반드시 `spec` 항목이 있어야 하며 하위에 반드시 `myvalue`라는 항목이 있어야 하고 이 값은 문자열이어야 함을 나타낸다.
* 이 조건을 모두 만족하는 커스텀 리소스의 YAML 파일은 아래와 같다.

```yaml
apiVersion: k106.com/v1alpha1
kind: Sshyeonzz
metadata:
  name: my-custom-resource
spec:
  myvalue: "hi!"
```

* CRD를 생성해서 커스텀 리소스의 사용을 쿠버네티스에 등록하면 커스텀 리소스를 생성할 수 있다.

##### 커스텀 리소스와 컨트롤러

* 커스텀 리소스를 생성했을 때 특정 동작을 수행하도록 정의하는 컨트롤러를 별도로 구현해야만 커스텀 리소스가 의미를 가진다. 
  * 예를 들어, 레플리카 셋의 목적은 라벨 셀렉터가 일치하는 일정 개수의 포드를 생성하는 것이고 이를 위한 동작은 컨트롤러 매니저라는 컴포넌트 내부에서 수행된다.
  * 이처럼 커스텀 리소스가 어떤 목적을 위해 생성되는지 비즈니스 로직으로 구현해 놓은 별도의 컨트롤러가 필요하다.
  * 이 비즈니스 로직은 커스텀 리소스가 원하는 바람직한 상태를 계속해서 유지하도록 만드는 소스코드로 구현되어야 한다.

* 커스텀 리소스의 동작 순서는 간략하게 다음과 같다.
  1. 커스텀 리소스를 정의하는 CRD 생성
  2. 커스텀 리소스 생성
  3. 커스텀 리소스를 위한 컨트롤러가 API 서버의 Watch를 통해 새로운 커스텀 리소스가 생성됐다는 것을 감지
  4. 커스텀 리소스가 원하는 바람직한 상태가 되도록 특정 동작 수행(**Reconcile**)
* 이러한 일련의 동작을 통해 CRD를 사용할 수 있도록 컨트롤러를 구현하는 방법을 오퍼레이터 패턴이라고 부르며, 쿠버네티스 기능 확장 시 중요하게 여겨지는 원리이기도 하다. 대부분의 오픈소스들은 리소스 관리의 복잡성을 줄이기 위해 오퍼레이터 패턴과 커스텀 리소스를 통해 사용할 수 있도록 제공된다.
  * 컨트롤러를 직접 구현하는 것은 매우 어려워 다양한 프레임워크가 제공된다.

##### 포드를 사용하는 다른 오브젝트들

##### 잡(Jobs)

* 잡은 특정 동작을 수행하고 종료해야 하는 작업을 위한 오브젝트이다. 포드를 생성해 원하는 동작을 수행한다는 점은 디플로이먼트와 같지만 잡에서 원하는 최종 상태는 **포드가 실행되어 정상적으로 종료되는 것**이다. 잡에서는 포드의 컨테이너가 종료 코드로 0을 반환하는 것을 목표로 한다.

```yaml
apiVersion: batch/v1
kind: Job # 잡
metadata:
  name: job-hello-world
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - image: busybox
        args: ["sh", "-c", "echo hello, World && exit 0"]
        name: job-hello-world
```

* 잡의 포드가 최종적으로 도달해야 하는 상태는 Completed이므로 포드의 `restartPolicy`를 명시적으로 `Never` 또는 `OnFailure`로 지정해 주어야 한다.

* 잡은 배치 작업을 위해서 쓰일 수 있다. 단, 잡은 동시성을 엄격하게 보장해야 하는 병렬 처리를 위해 사용하는 것은 아니다. 또한 잡의 포드가 실패하면 포드가 `restartPolicy`에 따라 재시작될 수 있어서 잡이 처리하는 작업은 몇 번을 수행해도 결과가 동일한 성질을 가지는 것이 좋다.

  * 쿠버네티스 공식 문서에는 YAML 템플릿을 이용해 동일한 잡을 여러 개 생성하거나, Message Queue나 Redis에 작업 큐를 저장해 둔 뒤 잡이 작업 큐를 꺼내와 처리하도록 하는 패턴 등을 설명하고 있다. [참조](https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-patterns)

* 잡에서 자주 사용되는 세부 옵션은 다음과 같다.

  * `spec.completions`: 잡이 성공했다고 여겨지려면 몇 개의 포드가 성공해야 하는지 설정
  * `spec.parallelism`: 동시에 생성될 포드의 개수를 설정

  > 작업이 진행되지 않고 막혀 있는 경우 성공이나 실패 아닌 상태로 오랜 시간 머무를 수 있으므로 포드가 실행될 수 있는 최대 시간을 `spec.activeDeadlineSeconds` 옵션으로 제한 가능

* 크론잡(CronJobs)은 잡을 주기적으로 실행하는 쿠버네티스 오브젝트로 특정 시간 간격으로 잡을 반복적으로 실행할 수 있다. 

  * 리눅스에서 흔히 쓰이는 크론의 스케줄 방법을 그대로 사용한다.

  ```yaml
  apiVersion: batch/v1beta1
  kind: CronJob
  metadata:
    name: cronjob-example
  spec:
    schedule: "*/1 * * * *" # Job 실행 주기
    jobTemplate:
      spec:
        template:
          spec:
            restartPolicy: Never
            containers:
            - name: cronjob-example
              image: busybox
              args: ["sh", "-c", "date"]
  ```

##### 데몬셋(Deamon Sets)

* 데몬셋은 쿠버네티스의 모든 노드에 동일한 포드를 하나씩 생성하는 오브젝트이다. 데몬셋은 로깅, 모니터링, 네트워킹 등을 위한 에이전트를 각 노드에 생성해야 할 때 유용하게 사용 가능하다.

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemonset-example
spec:
  selector:
    matchLabels:
      name: my-daemonset-example # 포드 생성하기 위한 셀렉터 설정
  template:
    metadata:
      labels:
        name: my-daemonset-example # 포드 라벨 설정
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master # 마스터 노드에 포드 생성
        effect: NoSchedule
      containers:
      - name: daemonset-example
        image: busybox
        args: ["tail", "-f", "/dev/null"]
        resources: # 자원 할당량 제한
          limits:
            cpu: 100m
            memory: 200Mi
```

* 데몬셋은 디플로이먼트처럼 라벨 셀렉터를 통해 포드를 생성한다. 따라서 라벨 셀렉터가 일치하는 포드를 같은 YAML 내에서 정한다.
* 데몬셋도 다른 오브젝트처럼 포드를 기본 단위로 사용하므로 마스터 노드에 설정되어 있는 Taint을 인식한 상태로 포드가 할당된다. 따라서 마스터 노드에도 포드를 생성하기 위해 간단한 Toleration을 설정한다.
  * taint는 노드에 정의할 수 있고, toleration은 포드에 정의할 수 있는데 taint 처리가 되어 있는 노드에는 포드가 배포되지 않는다. taint 처리가 되어 있는 노드에는 taint에 맞는 toleration을 가지고 있는 포드만 배포될 수 있다.
* 데몬셋은 일작적으로 노드에 대한 에이전트 역할을 하기 때문에 자원 부족으로 포드가 중지되는 것은 바람직하지 않으므로 데몬셋을 설정할 때에는 Guarenteed 클래스로 설정하는 것이 좋다.

##### 스테이트풀셋(StatefulSets)

* 상태를 갖는 포드를 관리하기 위한 오브젝트이다. 
* 쿠버네티스에서는 상태를 갖는 포드와 그렇지 않은 포드를 어떻게 다룰까?
  * 상태가 없는 포드를 지칭할 때는 흔히 '가축'에 비유한다. 목장에서 풀어 놓고 키우는 가축에 이름을 지어주지 않고 한 동물과 다른 동물을 특정 이름으로 구분하지도 않는다. 사람의 입장에서 가축은 언제든 대체될 수 있는 모든 동일해 보이는 개체이기 때문이다. 
  * 상태가 존재하는 포드를 지칭할 때는 '애완동물'에 비유한다. 애완동물은 특별한 이름을 붙여 주기 때문에 다른 애완동물과 구분된다. 대체 불가능한 개체로서 항상 고유한 식별자를 갖는 것으로 여겨진다. 

```yaml
apiVersion: apps/v1
kind: StatefuleSet
metadata:
  name: statefulset-example
spec:
  serviceName: statefulset-service
  selector:
    matchLabels:
      name: statefulset-example
  replicas: 3
  template:
    metadata:
      labels:
        name: statefulset-example
    spec:
      containers:
      - name: statefulset-example
        image: alicek106/rr-test:echo-hostname
        ports:
        - containerPort: 80
          name: web
---
apiVersion: v1
kind: Service
metadata:
  name: statefulset-service
spec:
  ports:
  - port: 80
    name: web
  clusterIP: none
  selector:
    name: statefulset-example
```

* 이를 생성하면 1개의 스테이드풀셋(3개 포드) 그리고 1개의 서비스가 생성된다.
  * 디플로이먼트에서 생성된 포드는 랜덤한 이름이 붙여지지만 스테이트풀셋으로부터 생성된 포드의 이름에는 숫자가 붙어 있다. 이 숫자를 통해 각 포드를 고유하게 식별한다.
* 다른 부분은 디플로이먼트와 크게 다르지 않지만 `serviceName`이라는 특이한 항목을 볼 수 있다. 왜 스테이트풀셋만 이 항목이 있을까?
  * 스테이트풀셋에서 생성되는 포드는 모두 고유하며 각자가 다른 개체로 취급되어야 한다. 이 경우 일반적인 서비스를 스테이트풀셋에 사용하면 서비스는 기본적으로 라벨 셀렉터가 일치하는 랜덤한 포드를 선택해 트래픽을 전달하므로 스테이트풀셋의 랜덤한 포드들에게 요청이 분산된다. 하지만 스테이트풀셋의 각 포드는 고유하게 식별돼야 하고 포드에 접근할 때에도 랜덤 포드가 아닌 개별 포드에 접근해야 할 것이다. 
  * 이 경우 일반적인 서비스가 아니라 헤드리스 서비스를 사용 가능하다. 헤드리스 서비스는 서비스의 이름으로 포드의 접근 위치를 알아내기 위해서 사용되며, 서비스의 이름과 포드의 이름을 통해서 포드에 직접 접근할 수 있다. `clusterIP` 항목의 값이 `None`으로 돼 있는 것이 헤드리스 서비스임을 의미한다.
  * 헤드리스 서비스의 이름은 SRV 레코드로 쓰이기 때문에 헤드리스 서비스의 이름을 통해 포드에 접근할 수 있는 IP를 반환할 수 있다. 혹은 `<포드의 이름>.<서비스의 이름>`을 통해서도 포드에 접근할 수 있다. 스테이트풀셋에서 포드의 이름에는 0부터 시작하는 숫자가 붙기 때문에 쿠버네티스 클러스터 내부에서는 고유한 포드 이름으로 접근 가능하다.

* 스테이트풀셋과 퍼시스턴스 볼륨
  * 스테이트풀셋은 보통 포드 내부에 데이터가 저장되는, 상태가 존재하는 애플리케이션을 배포할 것이다. 스테이트풀셋도 퍼시스턴트 볼륨을 포드에 마운트해 데이터를 보관하는 것이 바람직하다.
  * 포드가 여러 개라면 포드마다 퍼시스턴트 볼륨 클레임을 생성해 줘야 하는 번거로움이 있지만 쿠버네티스에서는 스테이트풀셋을 생성할 때 포드마다 퍼시스턴트 볼륨 클레임을 자동으로 생성함으로써 다이나믹 프로비저닝 기능을 사용할 수 있도록 지원한다.
  * 이 기능은 스테이트풀셋에서 `spec.volumeClaimTemplates` 항목을 정의함으로써 사용할 수 있다.

```yaml
apiVersion: apps/v1
kind: StatefuleSet
metadata:
  name: statefulset-example
spec:
  serviceName: statefulset-service
  selector:
    matchLabels:
      name: statefulset-example
  replicas: 3
  template:
    metadata:
      labels:
        name: statefulset-example
    spec:
      containers:
      - name: statefulset-example
        image: alicek106/rr-test:echo-hostname
        ports:
        - containerPort: 80
          name: web
---
apiVersion: v1
kind: Service
metadata:
  name: statefulset-service
spec:
  ports:
    - port: 80
      name: web
      volumeMounts:
      - name: webserver-files
        mountPath: /var/www/html
  clusterIP: none
  selector:
    name: statefulset-example
volumeClaimTemplates:
- metadata:
    name: webserver-files
  spec:
    accessModes: ["ReadWriteOnce"]
    storageClassName: standard
    resources:
      requests: 1Gi
      ...
```

* `volumeClaimTemplates`을 사용하면 스테이트풀셋의 각 포드에 대해 퍼시스턴트 볼륨 클레임이 생성된다. 각 포드마다 퍼시스턴트 볼륨이 동적으로 생성되는 것이다. 스테이트풀셋을 삭제한다고 해서 볼륨이 함께 삭제되지는 않으므로 퍼시스턴트 볼륨과 클레임은 직접 삭제해야 한다.

##### References

[시작하세요! 도커/쿠버네티스 (개정판)](https://wikibook.co.kr/docker-kube-rev/)